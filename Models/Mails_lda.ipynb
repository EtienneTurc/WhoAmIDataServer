{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json (r'gmail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" etienne voici ce que vous avez manque sur linkedin felicitez elise grenot pour ses 2 ans chez forum trium feliciter olivier castan a publie un postÂ\\xa0 exploit fully breaks sha1 lowers the attack bar commenter se desinscrireÂ\\xa0 aideÂ\\xa0 vous recevez des emails de notifications de linkedin cet email est destine a etienne turc etudiant en 3eme annee a l'ensta paristech decouvrez pourquoi nous precisons ceciÂ\\xa0 Â© 2020 linkedin ireland unlimited company wilton plaza wilton place dublin 2 linkedin est le nom commercial depose de linkedin ireland unlimited company linkedin et le logo de linkedin sont des marques deposees de linkedin\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tf_vect = tfidf.fit_transform(df.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01244576, 0.01244583, 0.01244576, 0.01246797, 0.88796364,\n",
       "        0.01244664, 0.01244575, 0.01244575, 0.01244714, 0.01244575],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81135485, 0.01272355,\n",
       "        0.01272347, 0.08685772, 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01243474, 0.01243474, 0.01243474, 0.88808719, 0.01243487,\n",
       "        0.01243479, 0.01243474, 0.01243474, 0.01243474, 0.01243474],\n",
       "       [0.01045181, 0.01045221, 0.01045239, 0.02114481, 0.89523882,\n",
       "        0.01045269, 0.01045181, 0.01045183, 0.01045181, 0.01045182],\n",
       "       [0.01305378, 0.01305378, 0.01305378, 0.80517563, 0.01305396,\n",
       "        0.01305385, 0.01305378, 0.09039388, 0.01305378, 0.01305378],\n",
       "       [0.01207562, 0.01207562, 0.01207563, 0.01208112, 0.80971562,\n",
       "        0.01207593, 0.01207562, 0.01207562, 0.01207562, 0.09367361],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.81745242, 0.01265014,\n",
       "        0.01265006, 0.08134741, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01260402, 0.01260402, 0.01260402, 0.88656365, 0.01260415,\n",
       "        0.01260408, 0.01260402, 0.01260402, 0.01260402, 0.01260402],\n",
       "       [0.01265   , 0.01265   , 0.01265   , 0.81766637, 0.01265013,\n",
       "        0.01265006, 0.08112927, 0.01265418, 0.01265   , 0.01265   ],\n",
       "       [0.01257927, 0.01257927, 0.01257927, 0.88678442, 0.01257939,\n",
       "        0.01258132, 0.01257927, 0.01257927, 0.01257927, 0.01257927],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.01346388, 0.01346408, 0.0134639 , 0.01348244, 0.87880423,\n",
       "        0.01346466, 0.01346516, 0.01346388, 0.01346388, 0.01346389],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81135485, 0.01272355,\n",
       "        0.01272347, 0.08685772, 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01339463, 0.01339464, 0.01339463, 0.74033796, 0.0134026 ,\n",
       "        0.01339553, 0.01339505, 0.01339553, 0.01339463, 0.15249481],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01177177, 0.01177178, 0.01177177, 0.53747189, 0.01177832,\n",
       "        0.01177785, 0.01177185, 0.01177184, 0.01177177, 0.36834115],\n",
       "       [0.01128888, 0.01128887, 0.01128888, 0.01129918, 0.74457839,\n",
       "        0.01128919, 0.01128887, 0.01128888, 0.16509997, 0.01128888],\n",
       "       [0.01260402, 0.01260402, 0.01260402, 0.88656365, 0.01260415,\n",
       "        0.01260408, 0.01260402, 0.01260402, 0.01260402, 0.01260402],\n",
       "       [0.01257203, 0.01257203, 0.01257203, 0.8252921 , 0.01257218,\n",
       "        0.01257209, 0.07413143, 0.01257203, 0.01257203, 0.01257203],\n",
       "       [0.01032732, 0.01032764, 0.01032736, 0.01089138, 0.01035094,\n",
       "        0.90646526, 0.01032732, 0.01032733, 0.01032813, 0.01032732],\n",
       "       [0.01105062, 0.01105064, 0.01105102, 0.01130143, 0.01105931,\n",
       "        0.90028447, 0.01105062, 0.01105063, 0.01105063, 0.01105062],\n",
       "       [0.01090373, 0.01090375, 0.01090374, 0.01120246, 0.01090619,\n",
       "        0.90156514, 0.01090373, 0.01090374, 0.01090379, 0.01090373],\n",
       "       [0.01251574, 0.01251574, 0.01251574, 0.88735815, 0.01251588,\n",
       "        0.0125158 , 0.01251574, 0.01251574, 0.01251574, 0.01251574],\n",
       "       [0.01158975, 0.01158976, 0.01158975, 0.01160102, 0.01159165,\n",
       "        0.89567906, 0.01158975, 0.01158976, 0.01158975, 0.01158975],\n",
       "       [0.01193231, 0.01193232, 0.01193231, 0.01194248, 0.01193421,\n",
       "        0.89259712, 0.01193231, 0.01193232, 0.01193231, 0.01193231],\n",
       "       [0.01326729, 0.01326729, 0.01326729, 0.73836359, 0.01326768,\n",
       "        0.01326734, 0.15549763, 0.01326729, 0.01326729, 0.01326729],\n",
       "       [0.01283776, 0.01283776, 0.01283776, 0.88445989, 0.01283793,\n",
       "        0.01283783, 0.01283776, 0.01283776, 0.01283776, 0.01283776],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.88614986, 0.01265013,\n",
       "        0.01265005, 0.01264999, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.88614986, 0.01265013,\n",
       "        0.01265005, 0.01264999, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01238637, 0.78668148, 0.01238652, 0.11411581, 0.01248967,\n",
       "        0.01239416, 0.01238642, 0.01238681, 0.0123864 , 0.01238637],\n",
       "       [0.01982718, 0.0198272 , 0.01982718, 0.01984104, 0.01982925,\n",
       "        0.8215394 , 0.01982719, 0.01982718, 0.01982718, 0.01982718],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01105649, 0.01105721, 0.01105655, 0.01632721, 0.0111004 ,\n",
       "        0.01105852, 0.5480197 , 0.01105649, 0.35820904, 0.01105839],\n",
       "       [0.01251574, 0.01251574, 0.01251574, 0.88735815, 0.01251588,\n",
       "        0.0125158 , 0.01251574, 0.01251574, 0.01251574, 0.01251574],\n",
       "       [0.01274796, 0.01274796, 0.01274796, 0.88526812, 0.01274812,\n",
       "        0.01274802, 0.01274796, 0.01274796, 0.01274796, 0.01274796],\n",
       "       [0.01200816, 0.01200816, 0.01200817, 0.01201296, 0.89192145,\n",
       "        0.01200844, 0.01200816, 0.01200816, 0.01200816, 0.01200817],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.88614986, 0.01265013,\n",
       "        0.01265005, 0.01264999, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01282968, 0.01282968, 0.01282968, 0.60842328, 0.0128301 ,\n",
       "        0.01282974, 0.17392309, 0.12784541, 0.01282968, 0.01282968],\n",
       "       [0.01214081, 0.01214102, 0.32055047, 0.01216703, 0.58229588,\n",
       "        0.0121414 , 0.01214086, 0.01214082, 0.01214084, 0.01214084],\n",
       "       [0.0108718 , 0.01087236, 0.01087556, 0.72099777, 0.19196533,\n",
       "        0.01087362, 0.01087179, 0.01087206, 0.01092791, 0.0108718 ],\n",
       "       [0.00925819, 0.67119664, 0.00925818, 0.0093153 , 0.00925814,\n",
       "        0.00925815, 0.00925874, 0.00925818, 0.25467244, 0.00926603],\n",
       "       [0.00904788, 0.00904798, 0.00904786, 0.00935523, 0.00905073,\n",
       "        0.00906692, 0.00904786, 0.00904787, 0.91823979, 0.00904787],\n",
       "       [0.00987742, 0.22537836, 0.00987885, 0.03152283, 0.6739455 ,\n",
       "        0.00988715, 0.0098775 , 0.00987749, 0.00987746, 0.00987745],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.01484512, 0.01484512, 0.01484512, 0.01491907, 0.86631848,\n",
       "        0.0148466 , 0.01484512, 0.01484512, 0.01484513, 0.01484512],\n",
       "       [0.01188836, 0.01188839, 0.01188836, 0.56979319, 0.07632426,\n",
       "        0.01189296, 0.01189017, 0.27065465, 0.01188836, 0.01189131],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.0130021 , 0.7601762 , 0.01300246, 0.1085413 , 0.04025757,\n",
       "        0.01301052, 0.0130021 , 0.0130025 , 0.01300315, 0.0130021 ],\n",
       "       [0.01763736, 0.01763743, 0.01763736, 0.01763736, 0.01763736,\n",
       "        0.01763736, 0.0176374 , 0.01763736, 0.67061479, 0.18828622],\n",
       "       [0.0127234 , 0.08683806, 0.0127234 , 0.8113745 , 0.01272355,\n",
       "        0.01272347, 0.0127234 , 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01507174, 0.01507338, 0.741464  , 0.13781889, 0.01516487,\n",
       "        0.0151194 , 0.01507196, 0.01507174, 0.01507228, 0.01507174],\n",
       "       [0.01214081, 0.01214102, 0.32055047, 0.01216703, 0.58229588,\n",
       "        0.0121414 , 0.01214086, 0.01214082, 0.01214084, 0.01214084],\n",
       "       [0.01289279, 0.01289309, 0.01289279, 0.01315209, 0.01294857,\n",
       "        0.01291913, 0.01289279, 0.01289279, 0.88362317, 0.01289279],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.81745242, 0.01265014,\n",
       "        0.01265006, 0.08134741, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01243474, 0.01243474, 0.01243474, 0.88808719, 0.01243487,\n",
       "        0.01243479, 0.01243474, 0.01243474, 0.01243474, 0.01243474],\n",
       "       [0.01297733, 0.01297733, 0.01297733, 0.81154143, 0.0129775 ,\n",
       "        0.01297739, 0.08463972, 0.01297733, 0.01297733, 0.01297733],\n",
       "       [0.01456668, 0.01456673, 0.01456668, 0.01456668, 0.01456668,\n",
       "        0.01456668, 0.01456668, 0.01456668, 0.86889985, 0.01456668],\n",
       "       [0.01257927, 0.01257927, 0.01257927, 0.88678442, 0.01257939,\n",
       "        0.01258132, 0.01257927, 0.01257927, 0.01257927, 0.01257927],\n",
       "       [0.01075467, 0.0107569 , 0.01075894, 0.01110876, 0.62181005,\n",
       "        0.01075619, 0.01075518, 0.29175996, 0.01078391, 0.01075545],\n",
       "       [0.00774166, 0.69515384, 0.00774177, 0.05861589, 0.19202792,\n",
       "        0.00774754, 0.00774408, 0.00774173, 0.00774316, 0.00774242],\n",
       "       [0.01282968, 0.01282968, 0.01282968, 0.60319508, 0.13374137,\n",
       "        0.01282974, 0.17325575, 0.01282968, 0.01282968, 0.01282968],\n",
       "       [0.01169449, 0.01169451, 0.01169449, 0.54292737, 0.01172095,\n",
       "        0.36348329, 0.0116947 , 0.01169485, 0.01169449, 0.01170086],\n",
       "       [0.01243474, 0.01243474, 0.01243474, 0.88808719, 0.01243487,\n",
       "        0.01243479, 0.01243474, 0.01243474, 0.01243474, 0.01243474],\n",
       "       [0.01251574, 0.01251574, 0.01251574, 0.88735815, 0.01251588,\n",
       "        0.0125158 , 0.01251574, 0.01251574, 0.01251574, 0.01251574],\n",
       "       [0.01238304, 0.01238304, 0.01238305, 0.01238867, 0.88854192,\n",
       "        0.01238334, 0.01238304, 0.01238304, 0.01238783, 0.01238305],\n",
       "       [0.01243474, 0.01243474, 0.01243474, 0.88808719, 0.01243487,\n",
       "        0.01243479, 0.01243474, 0.01243474, 0.01243474, 0.01243474],\n",
       "       [0.02088841, 0.81190994, 0.02088846, 0.02095658, 0.02088864,\n",
       "        0.02088939, 0.02088841, 0.02088841, 0.02091334, 0.02088841],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81078851, 0.01272355,\n",
       "        0.08742412, 0.0127234 , 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81135485, 0.01272355,\n",
       "        0.01272347, 0.08685772, 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01010206, 0.01010229, 0.01010235, 0.33906186, 0.11303217,\n",
       "        0.01011123, 0.47717697, 0.01010263, 0.01010207, 0.01010637],\n",
       "       [0.01251574, 0.01251574, 0.01251574, 0.88735815, 0.01251588,\n",
       "        0.0125158 , 0.01251574, 0.01251574, 0.01251574, 0.01251574],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81078851, 0.01272355,\n",
       "        0.08742412, 0.0127234 , 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01260402, 0.01260402, 0.01260402, 0.88656365, 0.01260415,\n",
       "        0.01260408, 0.01260402, 0.01260402, 0.01260402, 0.01260402],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81133567, 0.01272355,\n",
       "        0.01272347, 0.0127234 , 0.0868769 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01216419, 0.01216419, 0.14717545, 0.0121709 , 0.75550393,\n",
       "        0.01216455, 0.01216419, 0.01216419, 0.0121642 , 0.0121642 ],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01257203, 0.01257203, 0.01257203, 0.8252921 , 0.01257218,\n",
       "        0.01257209, 0.07413143, 0.01257203, 0.01257203, 0.01257203],\n",
       "       [0.01257203, 0.01257203, 0.01257203, 0.8252921 , 0.01257218,\n",
       "        0.01257209, 0.07413143, 0.01257203, 0.01257203, 0.01257203],\n",
       "       [0.01257203, 0.01257203, 0.01257203, 0.8252921 , 0.01257218,\n",
       "        0.01257209, 0.07413143, 0.01257203, 0.01257203, 0.01257203],\n",
       "       [0.01251574, 0.01251574, 0.01251574, 0.88735815, 0.01251588,\n",
       "        0.0125158 , 0.01251574, 0.01251574, 0.01251574, 0.01251574],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01167691, 0.01167967, 0.0116784 , 0.01184497, 0.66908703,\n",
       "        0.01167717, 0.01167691, 0.0116771 , 0.01167694, 0.23732489],\n",
       "       [0.01303148, 0.01304043, 0.65844683, 0.11132527, 0.13899463,\n",
       "        0.01303303, 0.01303148, 0.01303164, 0.01303366, 0.01303156],\n",
       "       [0.01242471, 0.01242471, 0.01242471, 0.69201731, 0.01242507,\n",
       "        0.01242476, 0.20858463, 0.01242471, 0.01242471, 0.01242471],\n",
       "       [0.01110218, 0.01110482, 0.01110834, 0.04606114, 0.22989412,\n",
       "        0.01110332, 0.01110217, 0.01110218, 0.64631937, 0.01110236],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01243474, 0.01243474, 0.01243474, 0.88808719, 0.01243487,\n",
       "        0.01243479, 0.01243474, 0.01243474, 0.01243474, 0.01243474],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81133567, 0.01272355,\n",
       "        0.01272347, 0.0127234 , 0.0868769 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.88548921, 0.01272353,\n",
       "        0.01272346, 0.0127234 , 0.0127234 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01250993, 0.01250993, 0.01250993, 0.88741038, 0.01251007,\n",
       "        0.01250999, 0.01250993, 0.01250993, 0.01250996, 0.01250993],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.81745137, 0.01265014,\n",
       "        0.01265006, 0.08134847, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.0127234 , 0.0127234 , 0.0127234 , 0.81133567, 0.01272355,\n",
       "        0.01272347, 0.0127234 , 0.0868769 , 0.0127234 , 0.0127234 ],\n",
       "       [0.01264999, 0.01264999, 0.01264999, 0.88614986, 0.01265013,\n",
       "        0.01265005, 0.01264999, 0.01264999, 0.01264999, 0.01264999],\n",
       "       [0.01052755, 0.0105293 , 0.01052754, 0.01075265, 0.01054972,\n",
       "        0.01052754, 0.0105278 , 0.01052755, 0.90500281, 0.01052755],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation()\n",
    "lda.fit_transform(tf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'print_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-501fe7353624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic: {} \\nWords: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'print_topics'"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1344 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 74 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vect[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c887f844748c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1f8cc8b77760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda_model_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic: {} Word: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no word id mapping provided; initializing from corpus, assuming identity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict_from_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mdict_from_corpus\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m     \"\"\"\n\u001b[1;32m--> 826\u001b[1;33m     \u001b[0mnum_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mget_max_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m     \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFakeDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mget_max_id\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[0mmaxid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m             \u001b[0mmaxid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfieldid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfieldid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaxid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             raise ValueError(\"The truth value of an array with more than one \"\n\u001b[0m\u001b[0;32m    288\u001b[0m                              \"element is ambiguous. Use a.any() or a.all().\")\n\u001b[0;32m    289\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(tf_vect, num_topics=10)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thepl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df.body.map(preprocess)\n",
    "\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3882340788212678),\n",
      " (1, 0.701846159861601),\n",
      " (2, 0.3007602581530492),\n",
      " (3, 0.3882340788212678),\n",
      " (4, 0.33985855158703643)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus_tfidf)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=40, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 1 \n",
      "Words: 0.894*\"anne\" + 0.015*\"depos\" + 0.015*\"nous\" + 0.015*\"plac\" + 0.015*\"turc\" + 0.015*\"discuss\" + 0.015*\"repons\" + 0.015*\"voir\"\n",
      "Topic: 2 \n",
      "Words: 0.498*\"repons\" + 0.485*\"discuss\" + 0.003*\"anne\" + 0.003*\"depos\" + 0.003*\"nous\" + 0.003*\"plac\" + 0.003*\"turc\" + 0.003*\"voir\"\n",
      "Topic: 3 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 4 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 5 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 6 \n",
      "Words: 0.861*\"nous\" + 0.109*\"voir\" + 0.005*\"anne\" + 0.005*\"depos\" + 0.005*\"plac\" + 0.005*\"turc\" + 0.005*\"discuss\" + 0.005*\"repons\"\n",
      "Topic: 7 \n",
      "Words: 0.496*\"repons\" + 0.482*\"discuss\" + 0.004*\"anne\" + 0.004*\"depos\" + 0.004*\"nous\" + 0.004*\"plac\" + 0.004*\"turc\" + 0.004*\"voir\"\n",
      "Topic: 8 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 9 \n",
      "Words: 0.485*\"repons\" + 0.472*\"discuss\" + 0.007*\"anne\" + 0.007*\"depos\" + 0.007*\"nous\" + 0.007*\"plac\" + 0.007*\"turc\" + 0.007*\"voir\"\n",
      "Topic: 10 \n",
      "Words: 0.434*\"repons\" + 0.423*\"discuss\" + 0.024*\"anne\" + 0.024*\"depos\" + 0.024*\"nous\" + 0.024*\"plac\" + 0.024*\"turc\" + 0.024*\"voir\"\n",
      "Topic: 11 \n",
      "Words: 0.338*\"voir\" + 0.239*\"depos\" + 0.170*\"nous\" + 0.067*\"anne\" + 0.067*\"plac\" + 0.061*\"turc\" + 0.039*\"discuss\" + 0.018*\"repons\"\n",
      "Topic: 12 \n",
      "Words: 0.282*\"depos\" + 0.169*\"plac\" + 0.169*\"anne\" + 0.152*\"turc\" + 0.138*\"nous\" + 0.030*\"discuss\" + 0.030*\"repons\" + 0.030*\"voir\"\n",
      "Topic: 13 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 14 \n",
      "Words: 0.279*\"depos\" + 0.186*\"plac\" + 0.150*\"turc\" + 0.141*\"anne\" + 0.131*\"nous\" + 0.110*\"voir\" + 0.001*\"discuss\" + 0.001*\"repons\"\n",
      "Topic: 15 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 16 \n",
      "Words: 0.503*\"repons\" + 0.489*\"discuss\" + 0.001*\"anne\" + 0.001*\"depos\" + 0.001*\"nous\" + 0.001*\"plac\" + 0.001*\"turc\" + 0.001*\"voir\"\n",
      "Topic: 17 \n",
      "Words: 0.278*\"depos\" + 0.174*\"anne\" + 0.163*\"plac\" + 0.144*\"turc\" + 0.128*\"nous\" + 0.071*\"voir\" + 0.021*\"discuss\" + 0.021*\"repons\"\n",
      "Topic: 18 \n",
      "Words: 0.941*\"voir\" + 0.008*\"anne\" + 0.008*\"depos\" + 0.008*\"nous\" + 0.008*\"plac\" + 0.008*\"turc\" + 0.008*\"discuss\" + 0.008*\"repons\"\n",
      "Topic: 19 \n",
      "Words: 0.649*\"turc\" + 0.100*\"voir\" + 0.097*\"depos\" + 0.086*\"nous\" + 0.017*\"anne\" + 0.017*\"plac\" + 0.017*\"discuss\" + 0.017*\"repons\"\n",
      "Topic: 20 \n",
      "Words: 0.434*\"repons\" + 0.423*\"discuss\" + 0.024*\"anne\" + 0.024*\"depos\" + 0.024*\"nous\" + 0.024*\"plac\" + 0.024*\"turc\" + 0.024*\"voir\"\n",
      "Topic: 21 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 22 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 23 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 24 \n",
      "Words: 0.470*\"repons\" + 0.457*\"discuss\" + 0.012*\"anne\" + 0.012*\"depos\" + 0.012*\"nous\" + 0.012*\"plac\" + 0.012*\"turc\" + 0.012*\"voir\"\n",
      "Topic: 25 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 26 \n",
      "Words: 0.261*\"depos\" + 0.180*\"voir\" + 0.175*\"turc\" + 0.158*\"nous\" + 0.103*\"plac\" + 0.096*\"anne\" + 0.014*\"discuss\" + 0.014*\"repons\"\n",
      "Topic: 27 \n",
      "Words: 0.500*\"repons\" + 0.486*\"discuss\" + 0.002*\"anne\" + 0.002*\"depos\" + 0.002*\"nous\" + 0.002*\"plac\" + 0.002*\"turc\" + 0.002*\"voir\"\n",
      "Topic: 28 \n",
      "Words: 0.330*\"anne\" + 0.310*\"voir\" + 0.183*\"plac\" + 0.036*\"depos\" + 0.036*\"nous\" + 0.036*\"turc\" + 0.036*\"discuss\" + 0.036*\"repons\"\n",
      "Topic: 29 \n",
      "Words: 0.439*\"turc\" + 0.210*\"depos\" + 0.164*\"nous\" + 0.122*\"voir\" + 0.028*\"plac\" + 0.028*\"anne\" + 0.005*\"discuss\" + 0.005*\"repons\"\n",
      "Topic: 30 \n",
      "Words: 0.352*\"nous\" + 0.260*\"repons\" + 0.254*\"discuss\" + 0.027*\"anne\" + 0.027*\"depos\" + 0.027*\"plac\" + 0.027*\"turc\" + 0.027*\"voir\"\n",
      "Topic: 31 \n",
      "Words: 0.487*\"repons\" + 0.474*\"discuss\" + 0.006*\"anne\" + 0.006*\"depos\" + 0.006*\"nous\" + 0.006*\"plac\" + 0.006*\"turc\" + 0.006*\"voir\"\n",
      "Topic: 32 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 33 \n",
      "Words: 0.269*\"depos\" + 0.205*\"turc\" + 0.187*\"voir\" + 0.182*\"nous\" + 0.071*\"plac\" + 0.071*\"anne\" + 0.007*\"discuss\" + 0.007*\"repons\"\n",
      "Topic: 34 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 35 \n",
      "Words: 0.497*\"repons\" + 0.483*\"discuss\" + 0.003*\"anne\" + 0.003*\"depos\" + 0.003*\"nous\" + 0.003*\"plac\" + 0.003*\"turc\" + 0.003*\"voir\"\n",
      "Topic: 36 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 37 \n",
      "Words: 0.434*\"repons\" + 0.423*\"discuss\" + 0.024*\"anne\" + 0.024*\"depos\" + 0.024*\"nous\" + 0.024*\"plac\" + 0.024*\"turc\" + 0.024*\"voir\"\n",
      "Topic: 38 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n",
      "Topic: 39 \n",
      "Words: 0.125*\"anne\" + 0.125*\"depos\" + 0.125*\"nous\" + 0.125*\"plac\" + 0.125*\"turc\" + 0.125*\"discuss\" + 0.125*\"repons\" + 0.125*\"voir\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
